{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias, en esta version, 1.6, no existe el SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==0.24 in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python2.7/site-packages (from pandas==0.24)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/site-packages (from pandas==0.24)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/site-packages (from pandas==0.24)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/site-packages (from python-dateutil>=2.5.0->pandas==0.24)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Instalamos pandas para poder escribir spark dfs a .csv\n",
    "!sudo pip install pandas==0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inicializar el SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inicializar el Hive Context\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Blue'>Creacion de DF por cada tabla en Hive</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vuelos = sqlContext.sql(\"SELECT * FROM ejercicio1.vuelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retrasos = sqlContext.sql(\"SELECT * FROM ejercicio1.retrasos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fecha = sqlContext.sql(\"SELECT * FROM ejercicio1.fecha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paises = sqlContext.sql(\"SELECT * FROM ejercicio1.paises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Blue'>3.Crear una nueva tabla en Hive, que contenga la siguiente información: Vuelo, Origen, Destino, Retraso.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+-------+\n",
      "|Vuelo|Origen|Destino|Retraso|\n",
      "+-----+------+-------+-------+\n",
      "| 6005|   MEX|    ESP|     52|\n",
      "| 2021|   COL|    MEX|     27|\n",
      "| 5528|   ARG|    COL|     36|\n",
      "| 7965|   BRA|    ARG|     82|\n",
      "| 8717|   USA|    BRA|     29|\n",
      "+-----+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Inner join de tablas Vuelos y Retrasos\n",
    "HD_ResumenVuelos = sqlContext.sql(\"SELECT v.Vuelo, v.Origen, v.Destino, r.Retraso FROM ejercicio1.vuelos v INNER JOIN ejercicio1.retrasos r ON (v.vuelo=r.vuelo)\")\n",
    "HD_ResumenVuelos.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Crear tabla HD_ResumenVuelos en la ruta hdfs del ejercicio\n",
    "HD_ResumenVuelos.write.option(\"path\", \"hdfs:/home/training/Desktop/Fuentes/HD_ResumenVuelos\").saveAsTable(\"ejercicio1.HD_ResumenVuelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Blue'>(A) ¿De qué país salieron más aviones?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(A) SQL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|          Pais|Salidas|\n",
      "+--------------+-------+\n",
      "|        España|      8|\n",
      "|       Francia|      7|\n",
      "|Estados Unidos|      6|\n",
      "+--------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Left join de Vuelos con Paises, con key orign y cod_pais, agrupado por pais y ordenado desc.\n",
    "HD_ResumenVuelosSalidasSQL = sqlContext.sql(\"SELECT p.Pais, COUNT(v.vuelo) As Salidas FROM ejercicio1.vuelos v LEFT JOIN ejercicio1.paises p ON (v.origen=p.cod_pais) GROUP BY p.Pais ORDER BY Salidas DESC\")\n",
    "HD_ResumenVuelosSalidasSQL.show(3)\n",
    "#De España(8) salieron mas vuelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(A) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          Pais|Vuelo|\n",
      "+--------------+-----+\n",
      "|        Mexico| 6005|\n",
      "|      Colombia| 2021|\n",
      "|     Argentina| 5528|\n",
      "|        Brasil| 7965|\n",
      "|Estados Unidos| 8717|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Crear DF HD_ResumenVuelosSalidas\n",
    "HD_ResumenVuelosSalidas = vuelos.alias(\"v\").join(paises.alias(\"p\"), vuelos.origen == paises.cod_pais)\n",
    "HD_ResumenVuelosSalidas = HD_ResumenVuelosSalidas.select(\"p.Pais\", \"v.Vuelo\")\n",
    "HD_ResumenVuelosSalidas.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|          Pais|Salidas|\n",
      "+--------------+-------+\n",
      "|        España|      8|\n",
      "|       Francia|      7|\n",
      "|Estados Unidos|      6|\n",
      "+--------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Left join de Vuelos con Paises, con key orign y cod_pais, agrupado por pais y ordenado desc.\n",
    "groupVuelos = HD_ResumenVuelosSalidas.groupBy(\"Pais\")\n",
    "groupVuelos = groupVuelos.agg({'Vuelo':'count'})\n",
    "groupVuelos = groupVuelos.withColumnRenamed(\"count(Vuelo)\", \"Salidas\")\n",
    "groupVuelos = groupVuelos.orderBy(\"Salidas\", ascending=False)\n",
    "groupVuelos.show(3)\n",
    "#De España(8) salieron mas vuelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Crear tabla HM_ResumenDias en la ruta hdfs del ejercicio\n",
    "groupVuelos.write.option(\"path\", \"hdfs:/home/training/Desktop/Fuentes/HD_ResumenVuelosSalidas\").saveAsTable(\"ejercicio1.HD_ResumenVuelosSalidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Blue'>(B)¿A qué país llegaron más aviones?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(B) SQL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  Pais|Llegadas|\n",
      "+------+--------+\n",
      "|Mexico|       7|\n",
      "| Rusia|       6|\n",
      "|  Peru|       6|\n",
      "+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Left join de Vuelos con Paises, con key destino y cod_pais, agrupado por pais y ordenado desc.\n",
    "HD_ResumenVuelosLlegadasSQL = sqlContext.sql(\"SELECT p.Pais, COUNT(v.vuelo) As Llegadas FROM ejercicio1.vuelos v LEFT JOIN ejercicio1.paises p ON (v.destino=p.cod_pais) GROUP BY p.Pais ORDER BY Llegadas DESC\")\n",
    "HD_ResumenVuelosLlegadasSQL.show(3)\n",
    "#A Mexico(7) llegaron mas vuelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(B) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     Pais|Vuelo|\n",
      "+---------+-----+\n",
      "|   España| 6005|\n",
      "|   Mexico| 2021|\n",
      "| Colombia| 5528|\n",
      "|Argentina| 7965|\n",
      "|   Brasil| 8717|\n",
      "+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Crear DF HD_ResumenVuelosLlegadas\n",
    "HD_ResumenVuelosLlegadas = vuelos.alias(\"v\").join(paises.alias(\"p\"), vuelos.destino == paises.cod_pais)\n",
    "HD_ResumenVuelosLlegadas = HD_ResumenVuelosLlegadas.select(\"p.Pais\", \"v.Vuelo\")\n",
    "HD_ResumenVuelosLlegadas.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  Pais|Llegadas|\n",
      "+------+--------+\n",
      "|Mexico|       7|\n",
      "| Rusia|       6|\n",
      "|  Peru|       6|\n",
      "+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Join de tablas Vuelos con Retrasos, grop by Destino, con key vuelo y ordenado ascendentemente\n",
    "groupVuelos = HD_ResumenVuelosLlegadas.groupBy(\"Pais\")\n",
    "groupVuelos = groupVuelos.agg({'Vuelo':'count'})\n",
    "groupVuelos = groupVuelos.withColumnRenamed(\"count(Vuelo)\", \"Llegadas\")\n",
    "groupVuelos = groupVuelos.orderBy(\"Llegadas\", ascending=False)\n",
    "groupVuelos.show(3)\n",
    "#A Mexico(7) llegaron mas vuelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Crear tabla HD_ResumenVuelosLlegadas en la ruta hdfs del ejercicio\n",
    "groupVuelos.write.option(\"path\", \"hdfs:/home/training/Desktop/Fuentes/HD_ResumenVuelosLlegadas\").saveAsTable(\"ejercicio1.HD_ResumenVuelosLlegadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Blue'>(C)¿Qué día hubo más vuelos? ¿Y menos?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(C) SQL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|Dia|Vuelos|\n",
      "+---+------+\n",
      "|  1|     7|\n",
      "|  9|     7|\n",
      "| 20|     7|\n",
      "|  5|     6|\n",
      "| 18|     6|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select de tabla Fecha, agroupado por dia, contando los vuelos \n",
    "HD_ResumenDiasSQL = sqlContext.sql(\"SELECT Dia, Count(Vuelo) as Vuelos FROM ejercicio1.fecha GROUP BY Dia ORDER BY Vuelos DESC\")\n",
    "HD_ResumenDiasSQL.show(5)\n",
    "#Dias 1, 9 y 20 tuvieron mas vuelos (7)\n",
    "\n",
    "HD_ResumenDiasSQLMenor = sqlContext.sql(\"SELECT Dia, Count(Vuelo) as Vuelos FROM ejercicio1.fecha GROUP BY Dia ORDER BY Vuelos ASC\")\n",
    "#HD_ResumenDiasSQLMenor.show(5)\n",
    "#Dias 23, 11, 17, 21 y 8 tuvieron menos vuelos (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(C) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|Dia|Vuelos|\n",
      "+---+------+\n",
      "|  1|     7|\n",
      "|  9|     7|\n",
      "| 20|     7|\n",
      "|  5|     6|\n",
      "| 18|     6|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select de tabla Fecha, agroupado por dia, contando los vuelos \n",
    "HD_ResumenDias = fecha.groupBy(\"Dia\")\n",
    "HD_ResumenDias = HD_ResumenDias.agg({'Vuelo':'count'})\n",
    "HD_ResumenDias = HD_ResumenDias.withColumnRenamed(\"count(Vuelo)\", \"Vuelos\")\n",
    "HD_ResumenDias = HD_ResumenDias.orderBy(\"Vuelos\", ascending=False)\n",
    "HD_ResumenDias.show(5)\n",
    "#Dias 1, 9 y 20 tuvieron mas vuelos (7)\n",
    "\n",
    "HD_ResumenDiasMenor = HD_ResumenDias.orderBy(\"Vuelos\", ascending=True)\n",
    "#HD_ResumenDiasMenor.show(5)\n",
    "#Dias 23, 11, 17, 21 y 8 tuvieron menos vuelos (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Crear tabla HD_ResumenDias en la ruta hdfs del ejercicio\n",
    "HD_ResumenDias.write.option(\"path\", \"hdfs:/home/training/Desktop/Fuentes/HD_ResumenDias\").saveAsTable(\"ejercicio1.HD_ResumenDias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Blue'>(D)¿Qué día hubo más retrasos? ¿Y menos?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(D) SQL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join de tabla Fecha on Retrasos con key vuelo, agrupar por dia y ordenar de forma asc/desc\n",
    "HD_ResumenRetrasosSQL = sqlContext.sql(\"SELECT f.Dia, Count(r.retraso) As Retrasos FROM ejercicio1.fecha f INNER JOIN ejercicio1.retrasos r ON (f.vuelo=r.vuelo) GROUP BY f.dia ORDER BY Retrasos DESC\")\n",
    "#El dia 20 de Febrero tuvo mas retrasos\n",
    "\n",
    "HD_ResumenRetrasosMenorSQL = sqlContext.sql(\"SELECT f.Dia, Count(r.retraso) As Retrasos FROM ejercicio1.fecha f INNER JOIN ejercicio1.retrasos r ON (f.vuelo=r.vuelo) GROUP BY f.dia ORDER BY Retrasos ASC\")\n",
    "#Los dias 23, 21, 17 y 11 de Febrero tuvo menos retrasos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|Dia|Retrasos|\n",
      "+---+--------+\n",
      "| 20|       8|\n",
      "|  1|       7|\n",
      "|  9|       7|\n",
      "|  5|       6|\n",
      "| 18|       6|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HD_ResumenRetrasosSQL.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(D) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|Dia|Retraso|\n",
      "+---+-------+\n",
      "|  2|     52|\n",
      "| 25|     27|\n",
      "| 18|     36|\n",
      "+---+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Crear tabla HD_ResumenRetrasos que contenga los min de retraso por dia \n",
    "HD_ResumenRetrasos = fecha.alias(\"f\").join(retrasos.alias(\"r\"), fecha.vuelo == retrasos.vuelo)\n",
    "HD_ResumenRetrasos = HD_ResumenRetrasos.select(\"f.Dia\",\"r.Retraso\")\n",
    "HD_ResumenRetrasos.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|Dia|Retrasos|\n",
      "+---+--------+\n",
      "| 20|       8|\n",
      "|  1|       7|\n",
      "|  9|       7|\n",
      "|  5|       6|\n",
      "| 18|       6|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Join de tablas Fecha con Retrasos, group by Dia, con key vuelo y ordenado ascendentemente\n",
    "groupRetrasos = HD_ResumenRetrasos.groupBy(\"Dia\")\n",
    "groupRetrasos = groupRetrasos.agg({'Retraso':'count'})\n",
    "groupRetrasos = groupRetrasos.withColumnRenamed(\"count(Retraso)\", \"Retrasos\")\n",
    "groupRetrasos = groupRetrasos.orderBy(\"Retrasos\", ascending=False)\n",
    "groupRetrasos.show(5)\n",
    "#El dia 20 de Febrero tuvo mas retrasos \n",
    "\n",
    "HD_ResumenRetrasosMenor = groupRetrasos.orderBy(\"Retrasos\", ascending=True)\n",
    "#HD_ResumenRetrasosMenor.show(5)\n",
    "#Los dias 23, 21, 17 y 11 de Febrero tuvieron menos retrasos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Crear tabla HD_ResumenRetrasos en la ruta hdfs del ejercicio\n",
    "groupRetrasos.write.option(\"path\", \"hdfs:/home/training/Desktop/Fuentes/HD_ResumenRetrasos\").saveAsTable(\"ejercicio1.HD_ResumenRetrasos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Guardar tablas generadas en .csv \n",
    "HD_ResumenVuelosSalidasSQL.toPandas().to_csv(\"/home/training/Desktop/EJERCICIO VIAJES/Fuentes Finales/HD_ResumenVuelosSalidas.csv\", encoding='utf-8')\n",
    "HD_ResumenVuelosLlegadasSQL.toPandas().to_csv(\"/home/training/Desktop/EJERCICIO VIAJES/Fuentes Finales/HD_ResumenVuelosLlegadas.csv\", encoding='utf-8')\n",
    "HD_ResumenDiasSQL.toPandas().to_csv(\"/home/training/Desktop/EJERCICIO VIAJES/Fuentes Finales/HD_ResumenDias.csv\", encoding='utf-8')\n",
    "HD_ResumenRetrasosSQL.toPandas().to_csv(\"/home/training/Desktop/EJERCICIO VIAJES/Fuentes Finales/HD_ResumenRetrasos.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Blue'>7.Convertir las operaciones realizadas para resolver el ejercicio 7 en un método definido por el alumno, que devuelva dicho resultado.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OperacionSQL:\n",
    "    def __init__ (self, *args):\n",
    "        self.dfs = list(args)\n",
    "        \n",
    "    def joinSQL(self, struc):\n",
    "        return self.dfs[0].alias(\"a\") \\\n",
    "               .join(self.dfs[1].alias(\"b\") \\\n",
    "               ,struc.id1==struc.id2,how=struc.tipo)\n",
    "        \n",
    "    def groupBySQL(self, struc, colAlias):\n",
    "        return self.dfs[0].groupBy(struc.col).agg({struc.aggCol:struc.aggOpe}) \\\n",
    "               .withColumnRenamed(\"{}({})\" \\\n",
    "               .format(struc.aggOpe, struc.aggCol), colAlias)\n",
    "        \n",
    "    def orderBySQL(self, col, sort):\n",
    "        order = sort\n",
    "        sort = False if order == \"desc\" else True\n",
    "        return self.dfs[0].orderBy(col, ascending=sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GroupByStructure:\n",
    "    def __init__ (self, col, aggCol, aggOpe):\n",
    "        self.col = col\n",
    "        self.aggCol = aggCol\n",
    "        self.aggOpe = aggOpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class JoinStructure:\n",
    "    def __init__ (self, tipo, id1, id2):\n",
    "        self.tipo = tipo.lower()\n",
    "        self.id1 = id1\n",
    "        self.id2 = id2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(7.A) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|          Pais|Salidas|\n",
      "+--------------+-------+\n",
      "|        España|      8|\n",
      "|       Francia|      7|\n",
      "|Estados Unidos|      6|\n",
      "+--------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ope1 = OperacionSQL(vuelos, paises)\n",
    "struc1 = JoinStructure(\"left\", vuelos.origen, paises.cod_pais)\n",
    "\n",
    "vuelosJoin = ope1.joinSQL(struc1) \\\n",
    "             .select(\"b.Pais\", \"a.Vuelo\")\n",
    "\n",
    "ope2 = OperacionSQL(vuelosJoin)\n",
    "struct2 = GroupByStructure(\"Pais\", \"Vuelo\", \"count\")\n",
    "\n",
    "vuelosGroup = ope2.groupBySQL(struct2, \"Salidas\")\n",
    "\n",
    "ope3 = OperacionSQL(vuelosGroup)\n",
    "vuelosOrder = ope3.orderBySQL(\"Salidas\", \"desc\")\n",
    "vuelosOrder.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(7.B) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  Pais|Llegadas|\n",
      "+------+--------+\n",
      "|Mexico|       7|\n",
      "| Rusia|       6|\n",
      "|  Peru|       6|\n",
      "+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ope1 = OperacionSQL(vuelos, paises)\n",
    "vuelosJoin = ope1.joinSQL(\"left\", vuelos.destino, paises.cod_pais) \\\n",
    "             .select(\"b.Pais\", \"a.Vuelo\")\n",
    "\n",
    "ope2 = OperacionSQL(vuelosJoin)\n",
    "struct1 = GroupByStructure(\"Pais\", \"Vuelo\", \"count\")\n",
    "\n",
    "vuelosGroup = ope2.groupBySQL(struct1, \"Llegadas\")\n",
    "\n",
    "ope3 = OperacionSQL(vuelosGroup)\n",
    "vuelosOrder = ope3.orderBySQL(\"Llegadas\", \"desc\")\n",
    "vuelosOrder.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(7.C) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|Dia|Vuelos|\n",
      "+---+------+\n",
      "| 23|     1|\n",
      "| 11|     1|\n",
      "| 17|     1|\n",
      "+---+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "struct1 = GroupByStructure(\"Dia\", \"Vuelo\", \"count\")\n",
    "ope1 = OperacionSQL(fecha)\n",
    "vuelosGroup = ope1.groupBySQL(struct1, \"Vuelos\")\n",
    "\n",
    "ope2 = OperacionSQL(vuelosGroup)\n",
    "#vuelosSorted = ope2.orderBySQL(\"Vuelos\", \"desc\")\n",
    "#vuelosSorted.show(3)\n",
    "\n",
    "vuelosSorted = ope2.orderBySQL(\"Vuelos\", \"asc\")\n",
    "vuelosSorted.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color: Green'>(7.D) PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|Dia|Retrasos|\n",
      "+---+--------+\n",
      "| 20|       8|\n",
      "|  9|       7|\n",
      "|  1|       7|\n",
      "|  5|       6|\n",
      "| 18|       6|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ope1 = OperacionSQL(fecha, retrasos)\n",
    "retrasosJoin = ope1.joinSQL(\"left\", fecha.vuelo, retrasos.vuelo) \\\n",
    "             .select(\"a.Dia\", \"b.Retraso\")\n",
    "\n",
    "ope2 = OperacionSQL(retrasosJoin)\n",
    "struct1 = GroupByStructure(\"Dia\", \"Retraso\", \"count\")\n",
    "\n",
    "retrasosGroup = ope2.groupBySQL(struct1, \"Retrasos\")\n",
    "\n",
    "ope3 = OperacionSQL(retrasosGroup)\n",
    "vuelosSorted = ope3.orderBySQL(\"Retrasos\", \"desc\")\n",
    "vuelosSorted.show(5)\n",
    "\n",
    "#vuelosSorted = ope3.orderBySQL(\"Retrasos\", \"asc\")\n",
    "#vuelosSorted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
